{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124fd0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.insert(0,'../..')\n",
    "import omama as O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baee957",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_scanner=O.deepsight_result_scanner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebecb6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "omama_loader = O.OmamaLoader()\n",
    "data = O.Data(data_loader=omama_loader, load_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86327edd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %load '/home/p.bendiksen001/deephealth/omama/omama/deep_sight/deep_sight_result_scanner.py'\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "sys.path.insert(0,'../..')\n",
    "import omama as O\n",
    "\n",
    "def list_pred_files(filepath, filetype):\n",
    "    depth_counter = 0\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        depth_counter += 1\n",
    "        # print(f\"root: {root}; dirs : {dirs} files: {files}\")\n",
    "        if '2d_out' not in root:\n",
    "            continue\n",
    "        if 'log.txt' not in files and 'predictions.json' not in files:\n",
    "            continue\n",
    "        for name in files:\n",
    "            # print(f\"counter {depth_counter}; name {name}\")\n",
    "            if name.lower().endswith(filetype.lower()):\n",
    "                json_path = os.path.join(root, name)\n",
    "                # print(f\"2: {json_path}\")\n",
    "                with open(json_path) as file:\n",
    "                    preds_dict = json.load(file)\n",
    "                # print(f\"preds: {preds_dict}\")\n",
    "                inner_total_counter=0\n",
    "                inner_error_counter=0\n",
    "                for key in preds_dict:\n",
    "                    inner_total_counter+=1\n",
    "                    updated_tot_count = total_counter[0] + 1\n",
    "                    total_counter.insert(0, updated_tot_count)\n",
    "                    total_counter.pop(1)\n",
    "                    if preds_dict[key][\"errors\"] != None:\n",
    "                        aggregate_errors_dict[key] = preds_dict[key][\"errors\"]\n",
    "                        inner_error_counter+=1\n",
    "                        updated_tot_count = total_counter_err[0] + 1\n",
    "                        total_counter_err.insert(0, updated_tot_count)\n",
    "                        total_counter_err.pop(1)\n",
    "                    else:\n",
    "                        aggregate_preds_dict[key] = {}\n",
    "                        aggregate_preds_dict[key][\"coords\"] = preds_dict[key][\"coords\"]\n",
    "                        aggregate_preds_dict[key][\"score\"] = preds_dict[key][\"score\"]\n",
    "                print(f\"error % for {name} is {inner_error_counter/inner_total_counter}\")\n",
    "                # paths.append(os.path.join(root, file))\n",
    "            elif name == 'caselist.txt':\n",
    "                print(os.getcwd())\n",
    "                with open(os.path.join(root, name), 'r') as fp:\n",
    "                    line_count=sum(1 for line in fp if line.rstrip())\n",
    "                    total_line_count=aggregate_num_lines[0]+line_count\n",
    "                    aggregate_num_lines.insert(0, total_line_count)\n",
    "                    aggregate_num_lines.pop(1)\n",
    "                    # print('Total lines:', aggregate_num_lines)  # 8\n",
    "    error_values = aggregate_errors_dict.values()\n",
    "    error_values = [item for sublist in error_values for item in sublist]\n",
    "    error_values = [item.rsplit(':')[0] for item in error_values]\n",
    "    print(f\"total error count : {total_counter_err[0]}\")\n",
    "    print(f\"total error %: {total_counter_err[0] / total_counter[0]}\")\n",
    "    # n, bins, patches = plt.hist(error_values)\n",
    "    # plt.xlabel(\"Values\")\n",
    "    # plt.ylabel(\"Frequency\")\n",
    "    # plt.title(\"Histogram\")\n",
    "    # plt.show()\n",
    "    len_preds = len(aggregate_preds_dict.keys())\n",
    "    len_errors = len(aggregate_errors_dict.keys())\n",
    "    len_preds_dict = {'case_count': len_preds, 'total': len_preds + len_errors }\n",
    "    len_preds_dict.update(aggregate_preds_dict)\n",
    "    len_errors_dict = {'case_count': len_errors, 'total': len_errors + len_preds}\n",
    "    len_errors_dict.update(aggregate_errors_dict)\n",
    "    # with open('/raid/mpsych/deepsight_run/2D_whitelist_results/2D_preds.txt', 'w') as file:\n",
    "    #     file.write(json.dumps(aggregate_preds_dict))  # use `json.loads` to do the reverse\n",
    "    # with open('/raid/mpsych/deepsight_run/2D_whitelist_results/2D_errs.txt', 'w') as file:\n",
    "    #     file.write(json.dumps(aggregate_errors_dict))  # use `json.loads` to do the reverse\n",
    "    print(f\"total cases: {aggregate_num_lines[0]}\")\n",
    "    return\n",
    "\n",
    "def get_label_by_sops(sop):\n",
    "    pass\n",
    "\n",
    "def Convert(lst):\n",
    "    new_dict = {}\n",
    "    for sublist in lst:\n",
    "        row=sublist\n",
    "        sop=row[0]\n",
    "        if sop in aggregate_preds_dict.keys():\n",
    "            print(\"ENTERED\")\n",
    "            new_dict[sop] = [aggregate_preds_dict[sop][\"score\"], row[2]]\n",
    "    print(f\"len heree: {len(new_dict.keys())}\")\n",
    "    return new_dict\n",
    "\n",
    "def find_matching_rows(filepath, dict_to_match, delimiter=',', quotechar='\"'):\n",
    "    matches = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        for name in files:\n",
    "            with open(os.path.join(root, name), 'r', newline='') as csvfile:\n",
    "                reader = csv.reader(csvfile, delimiter=',')\n",
    "                for row in reader:\n",
    "                    if row[0] in aggregate_preds_dict.keys():\n",
    "                        print(\"Matched Labels SOP with Pred SOP\")\n",
    "                    if row[0] == '2.25.100200417657065059753016354291204569362':\n",
    "                        print('Matched with Study UID')\n",
    "                    labels.append(row)\n",
    "    print(f\"Number of Labels cases: {len(labels)}\")\n",
    "    return Convert(labels)\n",
    "\n",
    "def cross_matirx(dict1, threshold):\n",
    "    tp=0\n",
    "    fp=0\n",
    "    tn=0\n",
    "    fn=0\n",
    "    counter=0\n",
    "    for sop in dict1.keys():\n",
    "        counter+=1\n",
    "        if sop_to_pred_label[sop][0] >= threshold and sop_to_pred_label[sop][1] != \"NonCancer\":\n",
    "            tp+=1\n",
    "        if sop_to_pred_label[sop][0] >= threshold and sop_to_pred_label[sop][1] == \"NonCancer\":\n",
    "            fp+=1\n",
    "        if sop_to_pred_label[sop][0] <= threshold and sop_to_pred_label[sop][1] == \"NonCancer\":\n",
    "            tn+=1\n",
    "        if sop_to_pred_label[sop][0] <= threshold and sop_to_pred_label[sop][1] != \"NonCancer\":\n",
    "            fn+=1\n",
    "    print(f\"tp: {tp}, fp: {fp}, tn: {tn}, fn: {fn} \\nPercentage: tp{tp/counter}, fp: {fp/counter}, tn: {tn/counter}, \"\n",
    "          f\"fn: {fn/counter} \")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    paths = []\n",
    "    aggregate_preds_dict = {}\n",
    "    aggregate_errors_dict = {}\n",
    "    num_lines=0\n",
    "    aggregate_num_lines = [0]\n",
    "    total_counter = [0]\n",
    "    total_counter_err = [0]\n",
    "    list_pred_files('/raid/mpsych/deepsight_run', '.json')\n",
    "    sop_to_pred_label = find_matching_rows(\"/raid/data01/deephealth/labels\", aggregate_preds_dict)\n",
    "    print(f\"len: {len(sop_to_pred_label)}\")\n",
    "    cross_matirx(sop_to_pred_label, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28869396",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239a322",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f21ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
