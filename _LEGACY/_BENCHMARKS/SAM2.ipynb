{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6992a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Sharpness-Aware-Minimization-TensorFlow' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e268a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"Sharpness-Aware-Minimization-TensorFlow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26a993d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0ff7f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of accelerators:  1\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "try: # detect TPUs\n",
    "    tpu = None\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1a1c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import resnet_cifar10\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9dbb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6740c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    return image, label\n",
    "\n",
    "def augment(image,label):\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 40, 40) # Add 8 pixels of padding\n",
    "    image = tf.image.random_crop(image, size=[32, 32, 3]) # Random crop back to 32x32\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
    "    image = tf.clip_by_value(image, 0., 1.)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(1024)\n",
    "    .map(scale, num_parallel_calls=AUTO)\n",
    "    .map(augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = (\n",
    "    test_ds\n",
    "    .map(scale, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ddd1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "class SAMModel(tf.keras.Model):\n",
    "    def __init__(self, resnet_model, rho=0.05):\n",
    "        \"\"\"\n",
    "        p, q = 2 for optimal results as suggested in the paper\n",
    "        (Section 2)\n",
    "        \"\"\"\n",
    "        super(SAMModel, self).__init__()\n",
    "        self.resnet_model = resnet_model\n",
    "        self.rho = rho\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (images, labels) = data\n",
    "        e_ws = []\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.resnet_model(images)\n",
    "            loss = self.compiled_loss(labels, predictions)\n",
    "        trainable_params = self.resnet_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_params)\n",
    "        grad_norm = self._grad_norm(gradients)\n",
    "        scale = self.rho / (grad_norm + 1e-12)\n",
    "\n",
    "        for (grad, param) in zip(gradients, trainable_params):\n",
    "            e_w = grad * scale\n",
    "            param.assign_add(e_w)\n",
    "            e_ws.append(e_w)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.resnet_model(images)\n",
    "            loss = self.compiled_loss(labels, predictions)    \n",
    "        \n",
    "        sam_gradients = tape.gradient(loss, trainable_params)\n",
    "        for (param, e_w) in zip(trainable_params, e_ws):\n",
    "            param.assign_sub(e_w)\n",
    "        \n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(sam_gradients, trainable_params))\n",
    "        \n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        (images, labels) = data\n",
    "        predictions = self.resnet_model(images, training=False)\n",
    "        loss = self.compiled_loss(labels, predictions)\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def _grad_norm(self, gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([\n",
    "                tf.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e5fc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5,\n",
    "        patience=3, verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4e2167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total learnable parameters: 0.575114 M\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = SAMModel(utils.get_training_model())\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "print(f\"Total learnable parameters: {model.resnet_model.count_params()/1e6} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67e2a499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 19s 35ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0999 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0993 - val_loss: nan - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0996 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0998 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: nan - accuracy: 0.1001 - val_loss: nan - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: nan - accuracy: 0.1002 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0999 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0998 - val_loss: nan - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: nan - accuracy: 0.0999 - val_loss: nan - val_accuracy: 0.1000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-28-29d015392523>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m history = model.fit(train_ds,\n\u001B[0m\u001B[1;32m      3\u001B[0m                    \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest_ds\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                    \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_callbacks\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                    epochs=100)\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1143\u001B[0m           \u001B[0mepoch_logs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1145\u001B[0;31m         \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1146\u001B[0m         \u001B[0mtraining_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mepoch_logs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1147\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_training\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_epoch_end\u001B[0;34m(self, epoch, logs)\u001B[0m\n\u001B[1;32m    430\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnumpy_logs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Only convert once.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    431\u001B[0m           \u001B[0mnumpy_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy_or_python_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 432\u001B[0;31m         \u001B[0mcallback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumpy_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    433\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    434\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_epoch_end\u001B[0;34m(self, epoch, logs)\u001B[0m\n\u001B[1;32m   1776\u001B[0m           \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Restoring model weights from the end of the best epoch.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1778\u001B[0;31m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1779\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1780\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_train_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36mset_weights\u001B[0;34m(self, weights)\u001B[0m\n\u001B[1;32m   1850\u001B[0m         \u001B[0mexpected_num_weights\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1851\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1852\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0mexpected_num_weights\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1853\u001B[0m       raise ValueError(\n\u001B[1;32m   1854\u001B[0m           \u001B[0;34m'You called `set_weights(weights)` on layer \"%s\" '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(train_ds,\n",
    "                   validation_data=test_ds,\n",
    "                   callbacks=train_callbacks,\n",
    "                   epochs=100)\n",
    "print(f\"Total training time: {(time.time() - start)/60.} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "327cecea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-f0f1f23b8f40>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot_history\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "603cf8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "391/391 [==============================] - 14s 22ms/step - loss: 2.3027 - accuracy: 0.0944 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.3027 - accuracy: 0.0932 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 2.3027 - accuracy: 0.0964 - val_loss: nan - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.3027 - accuracy: 0.0966 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.3027 - accuracy: 0.0954 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 2.3027 - accuracy: 0.0952 - val_loss: nan - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3026 - accuracy: 0.0959 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3026 - accuracy: 0.0963 - val_loss: nan - val_accuracy: 0.1000\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.3026 - accuracy: 0.0963 - val_loss: nan - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 2.3026 - accuracy: 0.0971 - val_loss: nan - val_accuracy: 0.1000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-ede2388d7a63>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m history = model.fit(train_ds,\n\u001B[0m\u001B[1;32m     10\u001B[0m                    \u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtest_ds\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m                    \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_callbacks\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1143\u001B[0m           \u001B[0mepoch_logs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1145\u001B[0;31m         \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1146\u001B[0m         \u001B[0mtraining_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mepoch_logs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1147\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_training\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_epoch_end\u001B[0;34m(self, epoch, logs)\u001B[0m\n\u001B[1;32m    430\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnumpy_logs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Only convert once.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    431\u001B[0m           \u001B[0mnumpy_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_numpy_or_python_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 432\u001B[0;31m         \u001B[0mcallback\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumpy_logs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    433\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    434\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001B[0m in \u001B[0;36mon_epoch_end\u001B[0;34m(self, epoch, logs)\u001B[0m\n\u001B[1;32m   1776\u001B[0m           \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Restoring model weights from the end of the best epoch.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1778\u001B[0;31m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1779\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1780\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mon_train_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/P3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36mset_weights\u001B[0;34m(self, weights)\u001B[0m\n\u001B[1;32m   1850\u001B[0m         \u001B[0mexpected_num_weights\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1851\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1852\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0mexpected_num_weights\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1853\u001B[0m       raise ValueError(\n\u001B[1;32m   1854\u001B[0m           \u001B[0;34m'You called `set_weights(weights)` on layer \"%s\" '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = utils.get_training_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_ds,\n",
    "                   validation_data=test_ds,\n",
    "                   callbacks=train_callbacks,\n",
    "                   epochs=200) # 200 eppochs since SAM takes two backprop steps for an update\n",
    "print(f\"Total training time: {(time.time() - start)/60.} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
