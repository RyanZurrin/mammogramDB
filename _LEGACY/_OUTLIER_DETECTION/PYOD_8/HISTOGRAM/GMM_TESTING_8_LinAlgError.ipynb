{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6c62a1",
   "metadata": {},
   "source": [
    "# GMM - Gaussian Mixture Model \n",
    "### Gaussian Mixture Models assume that each observation in a data set comes from a Gaussian Distribution with different mean and variance. By fitting the data to Gaussian Mixture Model, we aim to estimate the parameters of the gaussian distribution using the data.\n",
    "source: https://cmdlinetips.com/2021/03/gaussian-mixture-models-with-scikit-learn-in-python/\n",
    "\n",
    "source: https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341c331",
   "metadata": {},
   "source": [
    "### Arguments that work with GMM algorithm:\n",
    "1. n_components\n",
    "2. covariance_type\n",
    "3. tol\n",
    "4. reg_covar\n",
    "5. max_iter\n",
    "6. n_init\n",
    "7. init_params\n",
    "8. contamination\n",
    "9. weights_init\n",
    "10. means_init\n",
    "11. precisions_init\n",
    "12. random_state\n",
    "13. warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ea36f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import time\n",
    "sys.path.insert(0,'../../../..')\n",
    "import omama as O\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da64a66f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get2D                    ...took   225.059815 seconds\n"
     ]
    }
   ],
   "source": [
    "imgs = O.DataHelper.get2D(N = 100, config_num=5, randomize=True, timing=True)\n",
    "histograms = O.Features.get_features(imgs, feature_type=\"hist\", norm_type=\"minmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b66f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/sklearn/mixture/_gaussian_mixture.py\u001B[0m in \u001B[0;36m_compute_precision_cholesky\u001B[0;34m(covariances, covariance_type)\u001B[0m\n\u001B[1;32m    316\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 317\u001B[0;31m                 \u001B[0mcov_chol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcholesky\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcovariance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlower\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    318\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLinAlgError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\u001B[0m in \u001B[0;36mcholesky\u001B[0;34m(a, lower, overwrite_a, check_finite)\u001B[0m\n\u001B[1;32m     87\u001B[0m     \"\"\"\n\u001B[0;32m---> 88\u001B[0;31m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001B[0m\u001B[1;32m     89\u001B[0m                          check_finite=check_finite)\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/scipy/linalg/decomp_cholesky.py\u001B[0m in \u001B[0;36m_cholesky\u001B[0;34m(a, lower, overwrite_a, clean, check_finite)\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001B[0m\u001B[1;32m     38\u001B[0m                           \"definite\" % info)\n",
      "\u001B[0;31mLinAlgError\u001B[0m: 150-th leading minor of the array is not positive definite",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_110825/1112884208.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_scoresX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labelsX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mO\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOutlierDetector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetect_outliers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistograms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpyod_algorithm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'GMM'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mO\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview_image_and_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimgs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'hist'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_scores\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrain_scoresX\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Projects/omama/_EXPERIMENTS/OUTLIER_DETECTION/PYOD_5/HISTOGRAM/../../../../omama/outlier_detection/outlier_detector.py\u001B[0m in \u001B[0;36mdetect_outliers\u001B[0;34m(data_x, data_y, pyod_algorithm, activation_hidden, algorithm, alpha, approx_clf, approx_clf_list, approx_flag_global, approx_ng_clf_list, assume_centered, bandwidth, base_estimator, base_estimators, base_score, batch_size, behaviour, beta, booster, bootstrap, bootstrap_features, bps_flag, c, cache_size, capacity, check_detector, check_estimator, clustering_estimator, coef0, colsample_bylevel, colsample_bytree, combination, contamination, copy, cost_forecast_loc_fit, cost_forecast_loc_pred, covariance_type, D_layers, decay, decoder_neurons, degree, dis_measure, dropout_rate, encoder_neurons, epochs, epochs_query, eps, estimator_list, estimator_params, G_layers, gamma, hidden_activation, hidden_neurons, index_D_layer_for_recon_error, init_params, iterated_power, jl_method, kernel, l2_regularizer, latent_dim, latent_dim_G, leaf_size, learning_rate, learning_rate_query, loss, lr_d, lr_g, max_delta_step, max_depth, max_features, max_iter, max_samples, means_init, method, metric, metric_params, min_child_weight, momentum, n_bins, n_clusters, n_components, n_estimators, n_init, n_iter, n_jobs, n_neighbors, n_random_cuts, n_selected_components, novelty, nthread, nu, objective, optimizer, output_activation, p, parallel_execution, perplexity, precisions_init, preprocessing, radius, random_state, ref_set, reg_alpha, reg_covar, reg_lambda, rp_clf_list, rp_ng_clf_list, rp_flag_global, rule_of_thumb, scale_pos_weight, shrinking, silent, standardization, standardization_flag_list, stop_epochs, store_precision, subsample, subset_size, support_fraction, svd_solver, target_dim_frac, tol, use_ae, use_weights, validation_size, verbose, warm_start, weighted, weights_init, whiten, whitening)\u001B[0m\n\u001B[1;32m   1005\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1006\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdata_y\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1007\u001B[0;31m             \u001B[0mclf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1008\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1009\u001B[0m             \u001B[0mclf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/pyod/models/gmm.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    193\u001B[0m         )\n\u001B[1;32m    194\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 195\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetector_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m         \u001B[0;31m# invert decision_scores_. Outliers comes with higher outlier scores\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/sklearn/mixture/_base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    191\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m         \"\"\"\n\u001B[0;32m--> 193\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    194\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/sklearn/mixture/_base.py\u001B[0m in \u001B[0;36mfit_predict\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    237\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mdo_init\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 238\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    239\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    240\u001B[0m             \u001B[0mlower_bound\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfty\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mdo_init\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower_bound_\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/sklearn/mixture/_base.py\u001B[0m in \u001B[0;36m_initialize_parameters\u001B[0;34m(self, X, random_state)\u001B[0m\n\u001B[1;32m    155\u001B[0m                              % self.init_params)\n\u001B[1;32m    156\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 157\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    158\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    159\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mabstractmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/sklearn/mixture/_gaussian_mixture.py\u001B[0m in \u001B[0;36m_initialize\u001B[0;34m(self, X, resp)\u001B[0m\n\u001B[1;32m    648\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecisions_init\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    649\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcovariances_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcovariances\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 650\u001B[0;31m             self.precisions_cholesky_ = _compute_precision_cholesky(\n\u001B[0m\u001B[1;32m    651\u001B[0m                 covariances, self.covariance_type)\n\u001B[1;32m    652\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcovariance_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'full'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/O/lib/python3.9/site-packages/sklearn/mixture/_gaussian_mixture.py\u001B[0m in \u001B[0;36m_compute_precision_cholesky\u001B[0;34m(covariances, covariance_type)\u001B[0m\n\u001B[1;32m    317\u001B[0m                 \u001B[0mcov_chol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcholesky\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcovariance\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlower\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    318\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mlinalg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLinAlgError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 319\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mestimate_precision_error_message\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    320\u001B[0m             precisions_chol[k] = linalg.solve_triangular(cov_chol,\n\u001B[1;32m    321\u001B[0m                                                          \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meye\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_features\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar."
     ]
    }
   ],
   "source": [
    "train_scoresX, train_labelsX = O.OutlierDetector.detect_outliers(histograms, pyod_algorithm='GMM')\n",
    "O.Features.view_image_and_features(imgs, ['hist'], train_scores=[train_scoresX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173986b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
